# -*- coding: utf-8 -*-
"""Construction_Site_Captioning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kbaNH2qx7C_Szdj_70Z9Y8riPrJBWXjZ
"""

# !pip install streamlit

# ==============================
# STREAMLIT CIVIL CAPTION APP
# ==============================
import torch
from PIL import Image
import streamlit as st
from transformers import BlipProcessor, BlipForConditionalGeneration, CLIPProcessor, CLIPModel

# Device
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Load models
@st.cache_resource
def load_models():
    blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(DEVICE).eval()
    clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(DEVICE).eval()
    clip_proc = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    return blip_processor, blip_model, clip_proc, clip_model

blip_processor, blip_model, clip_proc, clip_model = load_models()

# Vocabulary
equipment_labels = [
    "bulldozer", "tower crane", "front loader", "hydraulic excavator",
    "backhoe loader", "soil compactor", "motor grader", "road roller",
    "asphalt paver", "cement mixer truck", "skid steer loader",
    "construction forklift", "dump truck", "drilling rig",
    "concrete pump truck", "wheel scraper"
]
action_phrases = [
    "digging soil", "lifting materials", "hauling debris",
    "paving road", "mixing concrete", "compacting ground",
    "grading surface", "pushing dirt", "loading truck",
    "pouring concrete", "drilling ground"
]

# Functions
@torch.inference_mode()
def generate_blip_caption(image: Image.Image) -> str:
    inputs = blip_processor(images=image, return_tensors="pt").to(DEVICE)
    out = blip_model.generate(**inputs, max_new_tokens=30)
    return blip_processor.decode(out[0], skip_special_tokens=True)

@torch.inference_mode()
def classify_equipment(image: Image.Image):
    inputs = clip_proc(text=equipment_labels, images=image, return_tensors="pt", padding=True).to(DEVICE)
    out = clip_model(**inputs)
    img_emb = out.image_embeds / out.image_embeds.norm(dim=-1, keepdim=True)
    txt_emb = out.text_embeds / out.text_embeds.norm(dim=-1, keepdim=True)
    sims = (img_emb @ txt_emb.T).squeeze(0)
    return equipment_labels[sims.argmax().item()]

@torch.inference_mode()
def pick_action(image: Image.Image):
    inputs = clip_proc(text=action_phrases, images=image, return_tensors="pt", padding=True).to(DEVICE)
    out = clip_model(**inputs)
    img_emb = out.image_embeds / out.image_embeds.norm(dim=-1, keepdim=True)
    txt_emb = out.text_embeds / out.text_embeds.norm(dim=-1, keepdim=True)
    sims = (img_emb @ txt_emb.T).squeeze(0)
    return action_phrases[sims.argmax().item()]

def refine_caption(raw_caption: str, equip: str, action: str) -> str:
    refined = raw_caption.lower()
    if equip not in refined:
        refined = f"{equip} at construction site"
    if action not in refined:
        refined = f"{refined}, {action}"
    return refined

# ==============================
# STREAMLIT UI
# ==============================
st.set_page_config(page_title="Construction Image Captioning", layout="centered")

st.title("🏗️ Construction Equipment Caption Generator")
st.markdown("Upload an image of construction equipment and get an **AI-generated caption** tailored to civil engineering context.")

uploaded_file = st.file_uploader("📤 Upload an Image", type=["jpg", "png", "jpeg"])

if uploaded_file:
    image = Image.open(uploaded_file).convert("RGB")
    st.image(image, caption="Uploaded Image", use_column_width=True)

    with st.spinner("🔍 Analyzing image..."):
        raw = generate_blip_caption(image)
        equip = classify_equipment(image)
        action = pick_action(image)
        tailored = refine_caption(raw, equip, action)

    st.success("✅ Caption Generated!")
    st.markdown(
        f"<h3 style='color:#1E90FF; text-align:center;'>📌 {tailored}</h3>",
        unsafe_allow_html=True
    )





