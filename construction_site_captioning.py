# -*- coding: utf-8 -*-
"""Construction_Site_Captioning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kbaNH2qx7C_Szdj_70Z9Y8riPrJBWXjZ
"""

# !pip install streamlit

# ==============================
# STREAMLIT CIVIL CAPTION APP
# ==============================
import streamlit as st
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch

# Load model + processor from Hugging Face Hub
model_name = "kneelabh87/blip-finetuned-construction_site_caption"

processor = BlipProcessor.from_pretrained(model_name)
model = BlipForConditionalGeneration.from_pretrained(model_name)

st.title("BLIP Fine-tuned Image Captioning")

uploaded_file = st.file_uploader("Upload an image", type=["jpg","jpeg","png"])

if uploaded_file:
    image = Image.open(uploaded_file).convert("RGB")
    st.image(image, caption="Uploaded Image", use_container_width=True)

    # Preprocess with processor (handles both tokenizer + feature extractor)
    inputs = processor(images=image, return_tensors="pt").to(device)
    pixel_values = inputs.pixel_values
    # Generate caption
    with torch.no_grad():
        output_ids = model.generate(pixel_values=pixel_values, max_length=1000)

    # Decode output
    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0])

    st.subheader("Generated Caption")
    st.write(caption)






