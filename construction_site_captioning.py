# -*- coding: utf-8 -*-
"""Construction_Site_Captioning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kbaNH2qx7C_Szdj_70Z9Y8riPrJBWXjZ
"""

# !pip install streamlit

# ==============================
# STREAMLIT CIVIL CAPTION APP
# ==============================
# -*- coding: utf-8 -*-
import streamlit as st
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch

# Load model + processor from Hugging Face Hub
model_name = "kneelabh87/blip-finetuned-construction_site_caption"

processor = BlipProcessor.from_pretrained(model_name)
model = BlipForConditionalGeneration.from_pretrained(model_name)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

st.title("Civil Construction Site Image Captioning")

uploaded_file = st.file_uploader("Upload an image", type=["jpg", "jpeg", "png"])

if uploaded_file:
    image = Image.open(uploaded_file).convert("RGB")
    st.image(image, caption="Uploaded Image", use_container_width=True)

    # Preprocess with processor
    inputs = processor(images=image, return_tensors="pt").to(device)

    # Generate caption
    with torch.no_grad():
    output_ids = model.generate(
        **inputs,
        min_length=50,
        max_length=200,
        num_beams=5,
        repetition_penalty=1.2,
        early_stopping=False
    )

     caption = processor.batch_decode(
    output_ids,
    skip_special_tokens=True,
    clean_up_tokenization_spaces=True
    )[0]   


    st.subheader("Generated Caption")
    st.write(caption)







